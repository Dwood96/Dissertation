{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bd7adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from here: https://medium.com/swlh/tweet-sentiment-analysis-using-python-for-complete-beginners-4aeb4456040\n",
    "# code also influenced from here: https://www.kaggle.com/code/ragnisah/text-data-cleaning-tweets-analysis/notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "Tweet_Data = pd.read_csv('Tweet_Data.csv')\n",
    "\n",
    "stop_words=stopwords.words('english')\n",
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d313d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50854, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tweet_Data.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb43004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50854, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_usernames_links(tweet):\n",
    "    tweet = re.sub('@[^\\s]+','',tweet)\n",
    "    tweet = re.sub('http[^\\s]+','',tweet)\n",
    "    return tweet\n",
    "Tweet_Data['Tweet'] = Tweet_Data['Tweet'].apply(remove_usernames_links)\n",
    "Tweet_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaf7ca08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j1/xwlyd2kd5hb4q6nbz5k6s9v80000gn/T/ipykernel_92533/2987164781.py:3: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50854, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code from here: https://s-hosseinkhani1999.medium.com/remove-all-kind-of-emojis-with-the-demoji-package-python-643a530491f4\n",
    "import demoji\n",
    "demoji.download_codes()\n",
    "def remove_em(text):\n",
    "    dem = demoji.findall(text)\n",
    "    for item in dem.keys():\n",
    "        text = text.replace(item, '')\n",
    "    return text\n",
    "Tweet_Data['Tweet'] = Tweet_Data['Tweet'].apply(remove_em)\n",
    "Tweet_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ccc1b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50854, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code taken from here:https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/\n",
    "\n",
    "#library that contains punctuation\n",
    "import string\n",
    "string.punctuation\n",
    "\n",
    "#defining the function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree\n",
    "#storing the puntuation free text\n",
    "Tweet_Data['clean_tweet']= Tweet_Data['Tweet'].apply(lambda x:remove_punctuation(x))\n",
    "Tweet_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0731077e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>View</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tweet_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-31 18:10:55+00:00</td>\n",
       "      <td>OwenJones84</td>\n",
       "      <td>yes yes it is</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>yes yes it is</td>\n",
       "      <td>yes yes it is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-12-31 18:05:32+00:00</td>\n",
       "      <td>OwenJones84</td>\n",
       "      <td>It has been an absolute shitter of a year, in multiple ways, and there's no getting around that....</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>It has been an absolute shitter of a year in multiple ways and theres no getting around that\\n\\n...</td>\n",
       "      <td>it has been an absolute shitter of a year in multiple ways and theres no getting around that\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-12-31 17:54:53+00:00</td>\n",
       "      <td>OwenJones84</td>\n",
       "      <td>hairdresser's away until 6th January, nightmare</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>hairdressers away until 6th January nightmare</td>\n",
       "      <td>hairdressers away until 6th january nightmare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-12-31 17:51:20+00:00</td>\n",
       "      <td>OwenJones84</td>\n",
       "      <td>#2009vs2019 \\n\\nTen years on and here I am, still looking like a budget Macaulay Culkin</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>2009vs2019 \\n\\nTen years on and here I am still looking like a budget Macaulay Culkin</td>\n",
       "      <td>2009vs2019 \\n\\nten years on and here i am still looking like a budget macaulay culkin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-12-31 16:41:34+00:00</td>\n",
       "      <td>OwenJones84</td>\n",
       "      <td>The most predictable thing in the world is anti-trans activists rushing to defend homophobic cra...</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>The most predictable thing in the world is antitrans activists rushing to defend homophobic crap...</td>\n",
       "      <td>the most predictable thing in the world is antitrans activists rushing to defend homophobic crap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       Date         User  \\\n",
       "0           0  2019-12-31 18:10:55+00:00  OwenJones84   \n",
       "1           1  2019-12-31 18:05:32+00:00  OwenJones84   \n",
       "2           2  2019-12-31 17:54:53+00:00  OwenJones84   \n",
       "3           3  2019-12-31 17:51:20+00:00  OwenJones84   \n",
       "4           4  2019-12-31 16:41:34+00:00  OwenJones84   \n",
       "\n",
       "                                                                                                 Tweet  \\\n",
       "0                                                                                        yes yes it is   \n",
       "1  It has been an absolute shitter of a year, in multiple ways, and there's no getting around that....   \n",
       "2                                                      hairdresser's away until 6th January, nightmare   \n",
       "3             #2009vs2019 \\n\\nTen years on and here I am, still looking like a budget Macaulay Culkin    \n",
       "4  The most predictable thing in the world is anti-trans activists rushing to defend homophobic cra...   \n",
       "\n",
       "      View  \\\n",
       "0  Liberal   \n",
       "1  Liberal   \n",
       "2  Liberal   \n",
       "3  Liberal   \n",
       "4  Liberal   \n",
       "\n",
       "                                                                                           clean_tweet  \\\n",
       "0                                                                                        yes yes it is   \n",
       "1  It has been an absolute shitter of a year in multiple ways and theres no getting around that\\n\\n...   \n",
       "2                                                        hairdressers away until 6th January nightmare   \n",
       "3               2009vs2019 \\n\\nTen years on and here I am still looking like a budget Macaulay Culkin    \n",
       "4  The most predictable thing in the world is antitrans activists rushing to defend homophobic crap...   \n",
       "\n",
       "                                                                                           tweet_lower  \n",
       "0                                                                                        yes yes it is  \n",
       "1  it has been an absolute shitter of a year in multiple ways and theres no getting around that\\n\\n...  \n",
       "2                                                        hairdressers away until 6th january nightmare  \n",
       "3               2009vs2019 \\n\\nten years on and here i am still looking like a budget macaulay culkin   \n",
       "4  the most predictable thing in the world is antitrans activists rushing to defend homophobic crap...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tweet_Data['tweet_lower']= Tweet_Data['clean_tweet'].apply(lambda x: x.lower())\n",
    "Tweet_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eace488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50854, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining function for tokenization\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize \n",
    "def tokenization(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "#applying function to the column\n",
    "Tweet_Data['tweet_tokenised']= Tweet_Data['tweet_lower'].apply(lambda x: tokenization(x))\n",
    "Tweet_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b73748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing nlp library\n",
    "import nltk\n",
    "#Stop words present in the library\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    output= [i for i in text if i not in stopwords]\n",
    "    return output\n",
    "\n",
    "Tweet_Data['no_stopwords']= Tweet_Data['tweet_tokenised'].apply(lambda x:remove_stopwords(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d0c10ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50854, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "#defining the object for stemming\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    stem_text = [porter_stemmer.stem(word) for word in text]\n",
    "    return stem_text\n",
    "Tweet_Data['tweet_stemmed']=Tweet_Data['no_stopwords'].apply(lambda x: stemming(x))\n",
    "Tweet_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47211f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50854, 11)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#defining the function for lemmatization\n",
    "def lemmatizer(text):\n",
    "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemm_text\n",
    "Tweet_Data['tweet_lemmatized']=Tweet_Data['no_stopwords'].apply(lambda x:lemmatizer(x))\n",
    "Tweet_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eebd7342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "training_docs = Tweet_Data['tweet_lemmatized']\n",
    "model= Word2Vec(training_docs,size=300,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35852c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function taken from here: https://github.com/PradipNichite/Youtube-Tutorials/blob/main/Yotutube_WordVectors.ipynb\n",
    "def sent_vec_corpus(sent):\n",
    "    vector_size = model.wv.vector_size\n",
    "    model.wv_res = np.zeros(vector_size)\n",
    "    # print(wv_res)\n",
    "    ctr = 1\n",
    "    for w in sent:\n",
    "        if w in model.wv:\n",
    "            ctr += 1\n",
    "            model.wv_res += model.wv[w]\n",
    "    model.wv_res = model.wv_res/ctr\n",
    "    return model.wv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fc1f820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [0.20818471908569336, -0.3044995864232381, 0.017170158525307972, 0.18408364057540894, -0.3491394...\n",
      "1        [0.12491666475155701, -0.1480059965979308, -0.01796685206257583, 0.12862281793301614, -0.1037743...\n",
      "2        [0.03663999186828733, -0.04670585785061121, 0.002301126392558217, 0.031321539729833606, -0.03373...\n",
      "3        [0.1574381380341947, -0.15598000373159135, 0.055554031394422054, 0.06790702718509627, -0.0557207...\n",
      "4        [0.034517355069207646, -0.11943318322300911, 0.05595608373793463, 0.023727972849479154, -0.08448...\n",
      "                                                        ...                                                 \n",
      "50849    [-0.023258453840389846, -0.08174765892326832, -0.10858170306310058, 0.12391643971204758, -0.0295...\n",
      "50850    [0.1837572589283809, -0.2890149224549532, 0.08148819714551792, 0.16420193493831903, -0.298969227...\n",
      "50851    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "50852    [0.2600117325782776, -0.2930988868077596, 0.0278716782728831, 0.14024271070957184, -0.1893314421...\n",
      "50853    [0.1485886424779892, -0.1408063918352127, 0.011536958627402782, 0.08306308090686798, -0.11998010...\n",
      "Name: trained_vec, Length: 50854, dtype: object\n"
     ]
    }
   ],
   "source": [
    "Tweet_Data['trained_vec'] = Tweet_Data['tweet_lemmatized'].apply(sent_vec_corpus)\n",
    "print(Tweet_Data['trained_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e312ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>View</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tweet_lower</th>\n",
       "      <th>tweet_tokenised</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "      <th>trained_vec</th>\n",
       "      <th>Conservative</th>\n",
       "      <th>Liberal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-31 18:10:55+00:00</td>\n",
       "      <td>OwenJones84</td>\n",
       "      <td>yes yes it is</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>yes yes it is</td>\n",
       "      <td>yes yes it is</td>\n",
       "      <td>[yes, yes, it, is]</td>\n",
       "      <td>[yes, yes]</td>\n",
       "      <td>[ye, ye]</td>\n",
       "      <td>[yes, yes]</td>\n",
       "      <td>[0.20818471908569336, -0.3044995864232381, 0.017170158525307972, 0.18408364057540894, -0.3491394...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-12-31 18:05:32+00:00</td>\n",
       "      <td>OwenJones84</td>\n",
       "      <td>It has been an absolute shitter of a year, in multiple ways, and there's no getting around that....</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>It has been an absolute shitter of a year in multiple ways and theres no getting around that\\n\\n...</td>\n",
       "      <td>it has been an absolute shitter of a year in multiple ways and theres no getting around that\\n\\n...</td>\n",
       "      <td>[it, has, been, an, absolute, shitter, of, a, year, in, multiple, ways, and, theres, no, getting...</td>\n",
       "      <td>[absolute, shitter, year, multiple, ways, theres, getting, around, fought, world, thats, decent,...</td>\n",
       "      <td>[absolut, shitter, year, multipl, way, there, get, around, fought, world, that, decent, happi, h...</td>\n",
       "      <td>[absolute, shitter, year, multiple, way, there, getting, around, fought, world, thats, decent, h...</td>\n",
       "      <td>[0.12491666475155701, -0.1480059965979308, -0.01796685206257583, 0.12862281793301614, -0.1037743...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-12-31 17:54:53+00:00</td>\n",
       "      <td>OwenJones84</td>\n",
       "      <td>hairdresser's away until 6th January, nightmare</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>hairdressers away until 6th January nightmare</td>\n",
       "      <td>hairdressers away until 6th january nightmare</td>\n",
       "      <td>[hairdressers, away, until, 6th, january, nightmare]</td>\n",
       "      <td>[hairdressers, away, 6th, january, nightmare]</td>\n",
       "      <td>[hairdress, away, 6th, januari, nightmar]</td>\n",
       "      <td>[hairdresser, away, 6th, january, nightmare]</td>\n",
       "      <td>[0.03663999186828733, -0.04670585785061121, 0.002301126392558217, 0.031321539729833606, -0.03373...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-12-31 17:51:20+00:00</td>\n",
       "      <td>OwenJones84</td>\n",
       "      <td>#2009vs2019 \\n\\nTen years on and here I am, still looking like a budget Macaulay Culkin</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>2009vs2019 \\n\\nTen years on and here I am still looking like a budget Macaulay Culkin</td>\n",
       "      <td>2009vs2019 \\n\\nten years on and here i am still looking like a budget macaulay culkin</td>\n",
       "      <td>[2009vs2019, ten, years, on, and, here, i, am, still, looking, like, a, budget, macaulay, culkin]</td>\n",
       "      <td>[2009vs2019, ten, years, still, looking, like, budget, macaulay, culkin]</td>\n",
       "      <td>[2009vs2019, ten, year, still, look, like, budget, macaulay, culkin]</td>\n",
       "      <td>[2009vs2019, ten, year, still, looking, like, budget, macaulay, culkin]</td>\n",
       "      <td>[0.1574381380341947, -0.15598000373159135, 0.055554031394422054, 0.06790702718509627, -0.0557207...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-12-31 16:41:34+00:00</td>\n",
       "      <td>OwenJones84</td>\n",
       "      <td>The most predictable thing in the world is anti-trans activists rushing to defend homophobic cra...</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>The most predictable thing in the world is antitrans activists rushing to defend homophobic crap...</td>\n",
       "      <td>the most predictable thing in the world is antitrans activists rushing to defend homophobic crap...</td>\n",
       "      <td>[the, most, predictable, thing, in, the, world, is, antitrans, activists, rushing, to, defend, h...</td>\n",
       "      <td>[predictable, thing, world, antitrans, activists, rushing, defend, homophobic, crap, press, thro...</td>\n",
       "      <td>[predict, thing, world, antitran, activist, rush, defend, homophob, crap, press, throw, grotesqu...</td>\n",
       "      <td>[predictable, thing, world, antitrans, activist, rushing, defend, homophobic, crap, press, throw...</td>\n",
       "      <td>[0.034517355069207646, -0.11943318322300911, 0.05595608373793463, 0.023727972849479154, -0.08448...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50849</th>\n",
       "      <td>274</td>\n",
       "      <td>2019-01-04 09:55:03+00:00</td>\n",
       "      <td>MelanieLatest</td>\n",
       "      <td>The pathological animus of the New York Times</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>The pathological animus of the New York Times</td>\n",
       "      <td>the pathological animus of the new york times</td>\n",
       "      <td>[the, pathological, animus, of, the, new, york, times]</td>\n",
       "      <td>[pathological, animus, new, york, times]</td>\n",
       "      <td>[patholog, animu, new, york, time]</td>\n",
       "      <td>[pathological, animus, new, york, time]</td>\n",
       "      <td>[-0.023258453840389846, -0.08174765892326832, -0.10858170306310058, 0.12391643971204758, -0.0295...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50850</th>\n",
       "      <td>275</td>\n",
       "      <td>2019-01-02 09:49:31+00:00</td>\n",
       "      <td>MelanieLatest</td>\n",
       "      <td>Eh? Have never even written about any trans woman. Think you’ve confused me with someone else, ...</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>Eh Have never even written about any trans woman Think you’ve confused me with someone else lik...</td>\n",
       "      <td>eh have never even written about any trans woman think you’ve confused me with someone else lik...</td>\n",
       "      <td>[eh, have, never, even, written, about, any, trans, woman, think, you, ’, ve, confused, me, with...</td>\n",
       "      <td>[eh, never, even, written, trans, woman, think, ’, confused, someone, else, like, seem, traduced...</td>\n",
       "      <td>[eh, never, even, written, tran, woman, think, ’, confus, someon, els, like, seem, traduc, time,...</td>\n",
       "      <td>[eh, never, even, written, trans, woman, think, ’, confused, someone, else, like, seem, traduced...</td>\n",
       "      <td>[0.1837572589283809, -0.2890149224549532, 0.08148819714551792, 0.16420193493831903, -0.298969227...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50851</th>\n",
       "      <td>276</td>\n",
       "      <td>2019-01-01 21:15:24+00:00</td>\n",
       "      <td>MelanieLatest</td>\n",
       "      <td></td>\n",
       "      <td>Conservative</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50852</th>\n",
       "      <td>277</td>\n",
       "      <td>2019-01-01 21:13:59+00:00</td>\n",
       "      <td>MelanieLatest</td>\n",
       "      <td>Thanks so much!</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>Thanks so much</td>\n",
       "      <td>thanks so much</td>\n",
       "      <td>[thanks, so, much]</td>\n",
       "      <td>[thanks, much]</td>\n",
       "      <td>[thank, much]</td>\n",
       "      <td>[thanks, much]</td>\n",
       "      <td>[0.2600117325782776, -0.2930988868077596, 0.0278716782728831, 0.14024271070957184, -0.1893314421...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50853</th>\n",
       "      <td>278</td>\n",
       "      <td>2019-01-01 21:13:36+00:00</td>\n",
       "      <td>MelanieLatest</td>\n",
       "      <td>Thank you!</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>Thank you</td>\n",
       "      <td>thank you</td>\n",
       "      <td>[thank, you]</td>\n",
       "      <td>[thank]</td>\n",
       "      <td>[thank]</td>\n",
       "      <td>[thank]</td>\n",
       "      <td>[0.1485886424779892, -0.1408063918352127, 0.011536958627402782, 0.08306308090686798, -0.11998010...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50854 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                       Date           User  \\\n",
       "0               0  2019-12-31 18:10:55+00:00    OwenJones84   \n",
       "1               1  2019-12-31 18:05:32+00:00    OwenJones84   \n",
       "2               2  2019-12-31 17:54:53+00:00    OwenJones84   \n",
       "3               3  2019-12-31 17:51:20+00:00    OwenJones84   \n",
       "4               4  2019-12-31 16:41:34+00:00    OwenJones84   \n",
       "...           ...                        ...            ...   \n",
       "50849         274  2019-01-04 09:55:03+00:00  MelanieLatest   \n",
       "50850         275  2019-01-02 09:49:31+00:00  MelanieLatest   \n",
       "50851         276  2019-01-01 21:15:24+00:00  MelanieLatest   \n",
       "50852         277  2019-01-01 21:13:59+00:00  MelanieLatest   \n",
       "50853         278  2019-01-01 21:13:36+00:00  MelanieLatest   \n",
       "\n",
       "                                                                                                     Tweet  \\\n",
       "0                                                                                            yes yes it is   \n",
       "1      It has been an absolute shitter of a year, in multiple ways, and there's no getting around that....   \n",
       "2                                                          hairdresser's away until 6th January, nightmare   \n",
       "3                 #2009vs2019 \\n\\nTen years on and here I am, still looking like a budget Macaulay Culkin    \n",
       "4      The most predictable thing in the world is anti-trans activists rushing to defend homophobic cra...   \n",
       "...                                                                                                    ...   \n",
       "50849                                                      The pathological animus of the New York Times     \n",
       "50850   Eh? Have never even written about any trans woman. Think you’ve confused me with someone else, ...   \n",
       "50851                                                                                                        \n",
       "50852                                                                                      Thanks so much!   \n",
       "50853                                                                                           Thank you!   \n",
       "\n",
       "               View  \\\n",
       "0           Liberal   \n",
       "1           Liberal   \n",
       "2           Liberal   \n",
       "3           Liberal   \n",
       "4           Liberal   \n",
       "...             ...   \n",
       "50849  Conservative   \n",
       "50850  Conservative   \n",
       "50851  Conservative   \n",
       "50852  Conservative   \n",
       "50853  Conservative   \n",
       "\n",
       "                                                                                               clean_tweet  \\\n",
       "0                                                                                            yes yes it is   \n",
       "1      It has been an absolute shitter of a year in multiple ways and theres no getting around that\\n\\n...   \n",
       "2                                                            hairdressers away until 6th January nightmare   \n",
       "3                   2009vs2019 \\n\\nTen years on and here I am still looking like a budget Macaulay Culkin    \n",
       "4      The most predictable thing in the world is antitrans activists rushing to defend homophobic crap...   \n",
       "...                                                                                                    ...   \n",
       "50849                                                      The pathological animus of the New York Times     \n",
       "50850   Eh Have never even written about any trans woman Think you’ve confused me with someone else lik...   \n",
       "50851                                                                                                        \n",
       "50852                                                                                       Thanks so much   \n",
       "50853                                                                                            Thank you   \n",
       "\n",
       "                                                                                               tweet_lower  \\\n",
       "0                                                                                            yes yes it is   \n",
       "1      it has been an absolute shitter of a year in multiple ways and theres no getting around that\\n\\n...   \n",
       "2                                                            hairdressers away until 6th january nightmare   \n",
       "3                   2009vs2019 \\n\\nten years on and here i am still looking like a budget macaulay culkin    \n",
       "4      the most predictable thing in the world is antitrans activists rushing to defend homophobic crap...   \n",
       "...                                                                                                    ...   \n",
       "50849                                                      the pathological animus of the new york times     \n",
       "50850   eh have never even written about any trans woman think you’ve confused me with someone else lik...   \n",
       "50851                                                                                                        \n",
       "50852                                                                                       thanks so much   \n",
       "50853                                                                                            thank you   \n",
       "\n",
       "                                                                                           tweet_tokenised  \\\n",
       "0                                                                                       [yes, yes, it, is]   \n",
       "1      [it, has, been, an, absolute, shitter, of, a, year, in, multiple, ways, and, theres, no, getting...   \n",
       "2                                                     [hairdressers, away, until, 6th, january, nightmare]   \n",
       "3        [2009vs2019, ten, years, on, and, here, i, am, still, looking, like, a, budget, macaulay, culkin]   \n",
       "4      [the, most, predictable, thing, in, the, world, is, antitrans, activists, rushing, to, defend, h...   \n",
       "...                                                                                                    ...   \n",
       "50849                                               [the, pathological, animus, of, the, new, york, times]   \n",
       "50850  [eh, have, never, even, written, about, any, trans, woman, think, you, ’, ve, confused, me, with...   \n",
       "50851                                                                                                   []   \n",
       "50852                                                                                   [thanks, so, much]   \n",
       "50853                                                                                         [thank, you]   \n",
       "\n",
       "                                                                                              no_stopwords  \\\n",
       "0                                                                                               [yes, yes]   \n",
       "1      [absolute, shitter, year, multiple, ways, theres, getting, around, fought, world, thats, decent,...   \n",
       "2                                                            [hairdressers, away, 6th, january, nightmare]   \n",
       "3                                 [2009vs2019, ten, years, still, looking, like, budget, macaulay, culkin]   \n",
       "4      [predictable, thing, world, antitrans, activists, rushing, defend, homophobic, crap, press, thro...   \n",
       "...                                                                                                    ...   \n",
       "50849                                                             [pathological, animus, new, york, times]   \n",
       "50850  [eh, never, even, written, trans, woman, think, ’, confused, someone, else, like, seem, traduced...   \n",
       "50851                                                                                                   []   \n",
       "50852                                                                                       [thanks, much]   \n",
       "50853                                                                                              [thank]   \n",
       "\n",
       "                                                                                             tweet_stemmed  \\\n",
       "0                                                                                                 [ye, ye]   \n",
       "1      [absolut, shitter, year, multipl, way, there, get, around, fought, world, that, decent, happi, h...   \n",
       "2                                                                [hairdress, away, 6th, januari, nightmar]   \n",
       "3                                     [2009vs2019, ten, year, still, look, like, budget, macaulay, culkin]   \n",
       "4      [predict, thing, world, antitran, activist, rush, defend, homophob, crap, press, throw, grotesqu...   \n",
       "...                                                                                                    ...   \n",
       "50849                                                                   [patholog, animu, new, york, time]   \n",
       "50850  [eh, never, even, written, tran, woman, think, ’, confus, someon, els, like, seem, traduc, time,...   \n",
       "50851                                                                                                   []   \n",
       "50852                                                                                        [thank, much]   \n",
       "50853                                                                                              [thank]   \n",
       "\n",
       "                                                                                          tweet_lemmatized  \\\n",
       "0                                                                                               [yes, yes]   \n",
       "1      [absolute, shitter, year, multiple, way, there, getting, around, fought, world, thats, decent, h...   \n",
       "2                                                             [hairdresser, away, 6th, january, nightmare]   \n",
       "3                                  [2009vs2019, ten, year, still, looking, like, budget, macaulay, culkin]   \n",
       "4      [predictable, thing, world, antitrans, activist, rushing, defend, homophobic, crap, press, throw...   \n",
       "...                                                                                                    ...   \n",
       "50849                                                              [pathological, animus, new, york, time]   \n",
       "50850  [eh, never, even, written, trans, woman, think, ’, confused, someone, else, like, seem, traduced...   \n",
       "50851                                                                                                   []   \n",
       "50852                                                                                       [thanks, much]   \n",
       "50853                                                                                              [thank]   \n",
       "\n",
       "                                                                                               trained_vec  \\\n",
       "0      [0.20818471908569336, -0.3044995864232381, 0.017170158525307972, 0.18408364057540894, -0.3491394...   \n",
       "1      [0.12491666475155701, -0.1480059965979308, -0.01796685206257583, 0.12862281793301614, -0.1037743...   \n",
       "2      [0.03663999186828733, -0.04670585785061121, 0.002301126392558217, 0.031321539729833606, -0.03373...   \n",
       "3      [0.1574381380341947, -0.15598000373159135, 0.055554031394422054, 0.06790702718509627, -0.0557207...   \n",
       "4      [0.034517355069207646, -0.11943318322300911, 0.05595608373793463, 0.023727972849479154, -0.08448...   \n",
       "...                                                                                                    ...   \n",
       "50849  [-0.023258453840389846, -0.08174765892326832, -0.10858170306310058, 0.12391643971204758, -0.0295...   \n",
       "50850  [0.1837572589283809, -0.2890149224549532, 0.08148819714551792, 0.16420193493831903, -0.298969227...   \n",
       "50851  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "50852  [0.2600117325782776, -0.2930988868077596, 0.0278716782728831, 0.14024271070957184, -0.1893314421...   \n",
       "50853  [0.1485886424779892, -0.1408063918352127, 0.011536958627402782, 0.08306308090686798, -0.11998010...   \n",
       "\n",
       "       Conservative  Liberal  \n",
       "0                 0        1  \n",
       "1                 0        1  \n",
       "2                 0        1  \n",
       "3                 0        1  \n",
       "4                 0        1  \n",
       "...             ...      ...  \n",
       "50849             1        0  \n",
       "50850             1        0  \n",
       "50851             1        0  \n",
       "50852             1        0  \n",
       "50853             1        0  \n",
       "\n",
       "[50854 rows x 14 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "views = pd.get_dummies(Tweet_Data['View'])\n",
    "Tweet_Data2 = Tweet_Data.join(views)\n",
    "Tweet_Data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "529c2df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([ 499.15358806, 1492.34156632, 2219.74478006,  580.86125326,\n",
      "       1090.08681703]), 'score_time': array([170.34050775, 178.61075568, 211.04377103, 182.54281282,\n",
      "       223.75830507]), 'test_accuracy': array([0.56621768, 0.62855176, 0.66276669, 0.57093698, 0.53588987]), 'test_precision': array([0.46912078, 0.60150376, 0.60072314, 0.47958643, 0.45448067]), 'test_recall': array([0.3663578 , 0.304038  , 0.55249406, 0.42969121, 0.6060822 ]), 'test_f1': array([0.41141942, 0.40391291, 0.5756001 , 0.45326986, 0.51944614])}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "svm = SVC()\n",
    "X = Tweet_Data2['trained_vec'].tolist()\n",
    "y = Tweet_Data2['Conservative']\n",
    "\n",
    "cv_results = cross_validate(svm, X, y, cv=5, scoring=('accuracy','precision','recall','f1'))\n",
    "print(cv_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b93b8e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/douglaswood/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/douglaswood/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/douglaswood/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/douglaswood/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([3.7403779 , 2.87768292, 4.28687501, 3.4126451 , 3.0539701 ]), 'score_time': array([0.08297491, 0.04229498, 0.05585408, 0.05405092, 0.06726193]), 'test_accuracy': array([0.51882804, 0.57880248, 0.61006784, 0.5694622 , 0.53048181]), 'test_precision': array([0.38062043, 0.48261278, 0.54335466, 0.47180514, 0.43494253]), 'test_recall': array([0.25944405, 0.24394299, 0.3631829 , 0.33586698, 0.44951295]), 'test_f1': array([0.30856174, 0.324077  , 0.43536446, 0.39239628, 0.44210772])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/douglaswood/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_regression = LogisticRegression(random_state=0)\n",
    "\n",
    "cv_logistic_results = cross_validate(logistic_regression, X, y, cv=5,scoring=('accuracy','precision','recall','f1'))\n",
    "print(cv_logistic_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d8807f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea9cb91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.11916804, 0.10040498, 0.09085894, 0.15206504, 0.09246397]), 'score_time': array([0.03842092, 0.03314376, 0.04152298, 0.04218864, 0.02896285]), 'test_accuracy': array([0.5081113 , 0.65971881, 0.64959198, 0.58568479, 0.47522124]), 'test_precision': array([0.40805929, 0.57059378, 0.55672638, 0.49965446, 0.42498005]), 'test_recall': array([0.41862675, 0.71900238, 0.75296912, 0.68693587, 0.75908767]), 'test_f1': array([0.41327548, 0.63625854, 0.6401454 , 0.5785157 , 0.54489639])}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "NB = ComplementNB()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "cv_NB_results = cross_validate(NB, X_scaled, y, cv=5,scoring=('accuracy','precision','recall','f1'))\n",
    "print(cv_NB_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93483265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feff1966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0967da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a30bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a005a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c98bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b473606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
